data=read.csv("group.csv", header=T)Apple=ts(data$APPLE, start=c(2007, 7), end=c(2014, 12), frequency=253)IBM=ts(data$IBM, start=c(2007, 7), end=c(2014, 12), frequency=253)Y=Applet=time(Apple)fit=lm(Y~t)summary(fit1)tsq=t^2/factorial(2)fit2=lm(Y~t+tsq)summary(fit2)##per=12t=1:length(Y)per=12c1=cos(2*pi*t/per)s1=sin(2*pi*t/per)apple_trig1=lm(Y~c1+s1)summary(apple_trig1)##per=365t=1:length(Y)per=365c1=cos(2*pi*t/per)s1=sin(2*pi*t/per)apple_trig1=lm(Y~c1+s1)summary(apple_trig1)t=1:length(Y)per=253c1=cos(2*pi*t/per)s1=sin(2*pi*t/per)apple_trig1=lm(Y~c1+s1)summary(apple_trig1)tcub= t^3/factorial(3)IBM linear fitlm(formula = Y ~ t)Coefficients:             Estimate Std. Error t value Pr(>|t|)    (Intercept) -3.617e+04  3.730e+02  -96.97   <2e-16 ***t            1.806e+01  1.855e-01   97.35   <2e-16 ***Residual standard error: 15.86 on 1775 degrees of freedomMultiple R-squared:  0.8423,	Adjusted R-squared:  0.8422 F-statistic:  9477 on 1 and 1775 DF,  p-value: < 2.2e-16IBM quadratic fitCall:lm(formula = Y ~ tsq)Coefficients:             Estimate Std. Error t value Pr(>|t|)    (Intercept) -1.801e+04  1.865e+02  -96.59   <2e-16 ***tsq          8.982e-03  9.227e-05   97.35   <2e-16 ***Residual standard error: 15.86 on 1775 degrees of freedomMultiple R-squared:  0.8422,	Adjusted R-squared:  0.8422 F-statistic:  9477 on 1 and 1775 DF,  p-value: < 2.2e-16## linear+quadraticCall:lm(formula = Y ~ t + tsq)Coefficients:             Estimate Std. Error t value Pr(>|t|)(Intercept) -6.443e+05  4.133e+05  -1.559    0.119t            6.230e+02  4.112e+02   1.515    0.130tsq         -3.009e-01  2.045e-01  -1.471    0.141Residual standard error: 15.85 on 1774 degrees of freedomMultiple R-squared:  0.8424,	Adjusted R-squared:  0.8423 F-statistic:  4743 on 2 and 1774 DF,  p-value: < 2.2e-16cubicCall:lm(formula = Y ~ tcub)Coefficients:             Estimate Std. Error t value Pr(>|t|)    (Intercept) -1.196e+04  1.243e+02  -96.21   <2e-16 ***tcub         8.935e-06  9.179e-08   97.34   <2e-16 ***Residual standard error: 15.86 on 1775 degrees of freedomMultiple R-squared:  0.8422,	Adjusted R-squared:  0.8421 F-statistic:  9476 on 1 and 1775 DF,  p-value: < 2.2e-16linear+quadratic+cubicCall:lm(formula = Y ~ t + tsq + tcub)Coefficients: (1 not defined because of singularities)             Estimate Std. Error t value Pr(>|t|)(Intercept) -6.443e+05  4.133e+05  -1.559    0.119t            6.230e+02  4.112e+02   1.515    0.130tsq         -3.009e-01  2.045e-01  -1.471    0.141tcub                NA         NA      NA       NAResidual standard error: 15.85 on 1774 degrees of freedomMultiple R-squared:  0.8424,	Adjusted R-squared:  0.8423 F-statistic:  4743 on 2 and 1774 DF,  p-value: < 2.2e-16linear+cubicCall:lm(formula = Y ~ t + tcub)Coefficients:             Estimate Std. Error t value Pr(>|t|)(Intercept) -4.443e+05  2.756e+05  -1.612    0.107t            3.226e+02  2.056e+02   1.569    0.117tcub        -1.507e-04  1.017e-04  -1.481    0.139Residual standard error: 15.85 on 1774 degrees of freedomMultiple R-squared:  0.8425,	Adjusted R-squared:  0.8423 F-statistic:  4743 on 2 and 1774 DF,  p-value: < 2.2e-16quadratic+cubicCall:lm(formula = Y ~ tsq + tcub)Coefficients:             Estimate Std. Error t value Pr(>|t|)  (Intercept) -2.295e+05  1.378e+05  -1.666   0.0959 .tsq          3.229e-01  2.045e-01   1.579   0.1145  tcub        -3.123e-04  2.034e-04  -1.535   0.1249  Residual standard error: 15.85 on 1774 degrees of freedomMultiple R-squared:  0.8425,	Adjusted R-squared:  0.8423 F-statistic:  4743 on 2 and 1774 DF,  p-value: < 2.2e-16quadratic&linear+quadraticAnalysis of Variance TableModel 1: Y ~ tsqModel 2: Y ~ t + tsq Res.Df    RSS Df Sum of Sq      F Pr(>F)1   1775 446249                           2   1774 445673  1    576.67 2.2954 0.1299seasonal period=365Call:lm(formula = Y ~ c1 + s1)Coefficients:           Estimate Std. Error t value Pr(>|t|)    (Intercept) 140.8503     0.9354 150.584  < 2e-16 ***c1           -3.8530     1.3340  -2.888  0.00392 ** s1           -8.1945     1.3114  -6.249 5.17e-10 ***Residual standard error: 39.4 on 1774 degrees of freedomMultiple R-squared:  0.02637,	Adjusted R-squared:  0.02527 F-statistic: 24.03 on 2 and 1774 DF,  p-value: 5.064e-11Call:lm(formula = Y ~ c1 + s1 + c2 + s2)Coefficients:           Estimate Std. Error t value Pr(>|t|)    (Intercept) 140.8015     0.9355 150.512  < 2e-16 ***c1           -3.9442     1.3347  -2.955  0.00317 ** s1           -8.1659     1.3110  -6.229 5.85e-10 ***c2           -2.5893     1.3220  -1.959  0.05032 .  s2            0.1919     1.3231   0.145  0.88473    ---Residual standard error: 39.38 on 1772 degrees of freedomMultiple R-squared:  0.02848,	Adjusted R-squared:  0.02629 F-statistic: 12.99 on 4 and 1772 DF,  p-value: 1.997e-10看出全部加起来时c2,s2不是significant，现在看c1+s1+c2Call:lm(formula = Y ~ c1 + s1 + c2)Coefficients:           Estimate Std. Error t value Pr(>|t|)    (Intercept) 140.8050     0.9349 150.610  < 2e-16 ***c1           -3.9381     1.3336  -2.953  0.00319 ** s1           -8.1693     1.3104  -6.234 5.66e-10 ***c2           -2.5858     1.3214  -1.957  0.05052 .  ---Residual standard error: 39.37 on 1773 degrees of freedomMultiple R-squared:  0.02847,	Adjusted R-squared:  0.02683 F-statistic: 17.32 on 3 and 1773 DF,  p-value: 4.38e-11Call:lm(formula = Y ~ c1 + s1 + s2)Coefficients:           Estimate Std. Error t value Pr(>|t|)    (Intercept) 140.8476     0.9359 150.489  < 2e-16 ***c1           -3.8576     1.3350  -2.890   0.0039 ** s1           -8.1920     1.3120  -6.244 5.32e-10 ***s2            0.1451     1.3240   0.110   0.9128    Residual standard error: 39.41 on 1773 degrees of freedomMultiple R-squared:  0.02638,	Adjusted R-squared:  0.02473 F-statistic: 16.01 on 3 and 1773 DF,  p-value: 2.842e-10这种情况下，我觉得可以删除c2和s2IBMdata=read.csv("group.csv", header=T)Apple=ts(data$APPLE, start=c(2007, 7), end=c(2014, 12), frequency=253)IBM=ts(data$IBM, start=c(2007, 7), end=c(2014, 12), frequency=253)Y=IBMtsq=t^2/factorial(2)tcub= t^3/factorial(3)fit1=lm(Y~t)n<-length(IBM)aic1<-AIC(fit1)/nk=2sse<-sum(resid(fit1)^2)aicc1<-log(sse/n)+((n+k)/(n-k-2))bic1<-(AIC(fit1,k=log(n)))/nfit2=lm(Y~tsq)n<-length(IBM)aic1<-AIC(fit2)/nk=2sse<-sum(resid(fit2)^2)aicc1<-log(sse/n)+((n+k)/(n-k-2))bic1<-(AIC(fit2,k=log(n)))/nfit3=lm(Y~t+tsq)n<-length(IBM)aic1<-AIC(fit3)/nk=3sse<-sum(resid(fit3)^2)aicc1<-log(sse/n)+((n+k)/(n-k-2))bic1<-(AIC(fit3,k=log(n)))/nfit4=lm(Y~tcub)n<-length(IBM)aic1<-AIC(fit4)/nk=2sse<-sum(resid(fit4)^2)aicc1<-log(sse/n)+((n+k)/(n-k-2))bic1<-(AIC(fit4,k=log(n)))/nfit5=lm(Y~t+tsq+tcub)n<-length(IBM)aic1<-AIC(fit5)/nk=4sse<-sum(resid(fit5)^2)aicc1<-log(sse/n)+((n+k)/(n-k-2))bic1<-(AIC(fit5,k=log(n)))/nfit6=lm(Y~t+tcub)n<-length(IBM)aic1<-AIC(fit6)/nk=3sse<-sum(resid(fit6)^2)aicc1<-log(sse/n)+((n+k)/(n-k-2))bic1<-(AIC(fit6,k=log(n)))/nfit7=lm(Y~tsq+tcub)n<-length(IBM)aic1<-AIC(fit7)/nk=3sse<-sum(resid(fit7)^2)aicc1<-log(sse/n)+((n+k)/(n-k-2))bic1<-(AIC(fit7,k=log(n)))/n##linearvalues=length(t)-100ts=t[1:values]IBM2=IBM[1:values]year.first<-values+1year.last<-length(t)IBM2.true=IBM[(values+1):length(t)]new<-data.frame(ts=t[year.first:year.last])fit1=lm(IBM2~ts)IBM2hat1=predict(fit1,new,se.fit=TRUE)IBM2.hat1=round(IBM2hat1$fit, 2)error=IBM2.true-IBM2.hat1me=mean(error)mpe=100*(mean(error/IBM2.true))mse=sum(error^2)/length(error)    mae=mean(abs(error))    mape=100*(mean(abs((error)/IBM2.true)))  ##t+tsqtsq<-(ts^2)/factorial(2)fit2<-lm(IBM2~ts+tsq)tsn<-t[year.first:year.last]tsnq<-(tsn^2)/factorial(2)mat<-cbind(tsn,tsnq)colnames(mat)<-c("ts","tsq")new2<-data.frame(mat)IBM2hat2<-predict(fit2,new2,se.fit=TRUE)IBM2.hat2<-round(IBM2hat2$fit,2)error2<-IBM2.true-IBM2.hat2me=mean(error2)mpe=100*(mean(error2/IBM2.true))mse=sum(error2^2)/length(error)    mae=mean(abs(error2))    mape=100*(mean(abs((error2)/IBM2.true)))t+tcubtscub<-(ts^3)/factorial(3)fit3<-lm(IBM2~ts+tscub)tcn<-t[year.first:year.last]tcnq<-(tcn^3)/factorial(3)mat2<-cbind(tcn,tcnq)colnames(mat2)<-c("ts","tscub")new3<-data.frame(mat2)IBM2hat3<-predict(fit9,new3,se.fit=TRUE) IBM2.hat3<-round(IBM2hat3$fit,2)error3<-IBM2.true-IBM2.hat3me=mean(error3)mpe=100*(mean(error3/IBM2.true))mse=sum(error3^2)/length(error)    mae=mean(abs(error3))    mape=100*(mean(abs((error3)/IBM2.true)))###diagonostic Y<-IBMt<-time(IBM)fit1=lm(Y~t)par(mfrow=c(2,2))plot(fit1,main="Fit Diagnostic",which=1:4)acf(IBM_res)acf(IBM_res,lag=150)acf plot cos and sin trend------seasonality. 